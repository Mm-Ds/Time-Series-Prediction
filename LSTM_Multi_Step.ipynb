{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM_multi_step.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QVn-9zKysvgL","colab_type":"code","colab":{}},"source":["from math import sqrt\n","from numpy import split\n","from numpy import array\n","from pandas import read_csv\n","from sklearn.metrics import mean_squared_error\n","from matplotlib import pyplot\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import LSTM\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHZiwMcROvAw","colab_type":"code","colab":{}},"source":["from tensorflow.python.client import device_lib      # uniquement pour vérifier que l'on est bien sur un environement d'execution GPU\n","print(device_lib.list_local_devices())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kr33UOmZeK1E","colab_type":"text"},"source":["# Fonctions\n","## Fractionnement de la série en ensembles train et test"]},{"cell_type":"code","metadata":{"id":"fjjPT0yN3j4z","colab_type":"code","colab":{}},"source":["# split a univariate dataset into train/test sets\n","def split_dataset(data):\n","  # split into weeks\n","  # split_indice=min( (dataset.index[dataset['dates'].str.contains(\"2018\")].tolist()))    # Il ne resulte pas forcément un nombre entier de blocs de 7 jours avec une telle séparation, d'où les traitements qui suivent (combler le datset et le choix des indices ici-bas)\n","  split_indice= 735          # les 105 premières semaines (<=> jusqu'au 05/01/2018 ) sont prises comme ensemble train et les 52 restantes (<=>à partir de 06/01/2018 ) comme ensemble test \n","  train,test = data[0:split_indice], data[split_indice:data.shape[0]]\n","  # restructure into windows of 7 days              \n","  train = array(split(train, len(train)/7))\n","  test = array(split(test, len(test)/7))\n","  return train, test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vLoxQYweoCD","colab_type":"text"},"source":["## Conversion de la série en format \"séquence-cible\" (en un format pour apprentissage supérvisé)"]},{"cell_type":"code","metadata":{"id":"PqeGrYV4JAMq","colab_type":"code","colab":{}},"source":["# convert history into inputs and outputs\n","def to_supervised(train, n_input, n_out=7):\n","\t# flatten data\n","\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","\tX, y = list(), list()\n","\tin_start = 0\n","\t# step over the entire history one time step at a time\n","\tfor _ in range(len(data)):\n","\t\t# define the end of the input sequence\n","\t\tin_end = in_start + n_input\n","\t\tout_end = in_end + n_out\n","\t\t# ensure we have enough data for this instance\n","\t\tif out_end <= len(data):\n","\t\t\tx_input = data[in_start:in_end, 1]              \n","\t\t\tx_input = x_input.reshape((len(x_input), 1))\n","\t\t\tX.append(x_input)\n","\t\t\ty.append(data[in_end:out_end, 1])\n","\t\t# move along one time step\n","\t\tin_start += 1\n","\treturn array(X), array(y)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EAhB165yfL67","colab_type":"text"},"source":["## Construction et entrainement du modèle"]},{"cell_type":"code","metadata":{"id":"hvAjVlltJJwf","colab_type":"code","colab":{}},"source":["# train the model\n","def build_model(train,config):\n","  # unpack configuration\n","  n_input,n_nodes, epochs, batch_size = config\n","  # prepare data\n","  train_x, train_y = to_supervised(train, n_input)            \n","  # print(train_x.shape)   # (722, 7, 1)  \n","  # print(train_y.shape)     # (722, 7)\n","  # define parameters \n","                                 \n","  n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n","  # define model\n","  model = Sequential()\n","  model.add(LSTM(n_nodes, activation='relu', input_shape=(n_timesteps, n_features)))\n","  model.add(Dense(100, activation='relu'))          \n","  model.add(Dense(n_outputs))\n","  model.compile(loss='mse', optimizer='adam')\n","  # fit network\n","  start=time.time()\n","  model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=5)\n","\n","  print(\"Temps d'entrainement =   %s\" %(time.time()-start))\n","  time.sleep(1)\n","  return model\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hJJjdaMQfS3r","colab_type":"text"},"source":["## Calcul des erreurs de prédiction"]},{"cell_type":"code","metadata":{"id":"MPbm_9PBa7kc","colab_type":"code","colab":{}},"source":["# evaluate one or more weekly forecasts against expected values\n","def evaluate_forecasts(actual, predicted):\n","\tscores = list()\n","\t# calculate an RMSE score for each day\n","\tfor i in range(actual.shape[1]):\n","\t\t# calculate mse\n","\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n","\t\t# calculate rmse\n","\t\trmse = sqrt(mse)\n","\t\t# store\n","\t\tscores.append(rmse)\n","\t# calculate overall RMSE\n","\ts = 0\n","\tfor row in range(actual.shape[0]):\n","\t\tfor col in range(actual.shape[1]):\n","\t\t\ts += (actual[row, col] - predicted[row, col])**2\n","\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n","\treturn score, scores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hJtdibjguQel","colab_type":"text"},"source":["## Affichage des scores"]},{"cell_type":"code","metadata":{"id":"krt0v-C3uOTi","colab_type":"code","colab":{}},"source":["# summarize scores\n","def summarize_scores(name, score, scores):\n","\ts_scores = ', '.join(['%.1f' % s for s in scores])\n","\tprint('Résumé scores : modèle : %s: score global [%.3f], scores partiels  %s' % (name, score, s_scores))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXd3mctZfmip","colab_type":"text"},"source":["## Prédiction "]},{"cell_type":"code","metadata":{"id":"29PbqThhJsty","colab_type":"code","colab":{}},"source":["# make a forecast\n","def forecast(model, history, n_input):\n","\t# flatten data\n","\tdata = array(history)\n","\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n","\t# retrieve last observations for input data\n","\tinput_x = data[-n_input:, 1]\n","\t# reshape into [1, n_input, 1]\n","\tinput_x = input_x.reshape((1, len(input_x), 1))\n","\t# forecast the next week\n","\tyhat = model.predict(input_x, verbose=0)\n","\t# we only want the vector forecast\n","\tyhat = yhat[0]\n","\treturn yhat"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3a1GNM84fpJo","colab_type":"text"},"source":["## Evaluation du modèle (validation \"walk-forward\" pour chaque bloc de 7 jours)"]},{"cell_type":"code","metadata":{"id":"CjAEjuOALC8R","colab_type":"code","colab":{}},"source":["# evaluate a single model\n","def evaluate_model(model,train, test, n_input):\n","\t# history is a list of weekly data\n","\thistory = [x for x in train]\n","\t# walk-forward validation over each week\n","\tpredictions = list()\n","\tfor i in range(len(test)):\n","\t\t# predict the week\n","\t\tyhat_sequence = forecast(model, history, n_input)\n","\t\t# store the predictions\n","\t\tpredictions.append(yhat_sequence)\n","\t\t# get real observation and add to history for predicting the next week\n","\t\thistory.append(test[i, :])\n","\t# evaluate predictions days for each week\n","\tpredictions = array(predictions)\n","\tscore, scores = evaluate_forecasts(test[:, :, 1], predictions)\n","\treturn score, scores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CyQT_IN3dHO1","colab_type":"text"},"source":["# Chargement des données"]},{"cell_type":"code","metadata":{"id":"YiEtGiACr1BG","colab_type":"code","colab":{}},"source":["dataset = read_csv('/content/drive/My Drive/TER/pass.csv', sep=';',usecols=[\"dates\",\"nb\"], low_memory=False, infer_datetime_format=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5G2fDD9ssuIX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"3158bd89-3dd3-4d16-9f02-e455d2e91326","executionInfo":{"status":"ok","timestamp":1591526912261,"user_tz":-120,"elapsed":955,"user":{"displayName":"myriam dehmas","photoUrl":"https://lh3.googleusercontent.com/-QiP9TxfNrPI/AAAAAAAAAAI/AAAAAAAAB-c/5Z3Xu-PaDwg/s64/photo.jpg","userId":"07724041903811261552"}}},"source":["print(dataset.shape)\n","print(dataset)\n","print(dataset[dataset.isna().any(axis=1)])"],"execution_count":28,"outputs":[{"output_type":"stream","text":["(1099, 2)\n","           dates    nb\n","0     01/01/2016  4618\n","1     02/01/2016  4925\n","2     03/01/2016  4325\n","3     04/01/2016  4528\n","4     05/01/2016  4134\n","...          ...   ...\n","1094  30/12/2018  4729\n","1095  31/12/2018  4251\n","1089  25/12/2018  4247\n","1090  26/12/2018  4715\n","1091  27/12/2018  4761\n","\n","[1099 rows x 2 columns]\n","Empty DataFrame\n","Columns: [dates, nb]\n","Index: []\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CTEEts5vdWNh","colab_type":"text"},"source":["# Combler le dernier bloc de 4 jours avec les 3 dernières valeurs du bloc de 7 jour précédent"]},{"cell_type":"code","metadata":{"id":"rV24ngFt_cRK","colab_type":"code","colab":{}},"source":["dataset= dataset.append(dataset.iloc[dataset.shape[0]-7:dataset.shape[0]-4,])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XHSHPwbA90h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"0d8127bf-e746-4409-c0b3-fb74b3a10f2d","executionInfo":{"status":"ok","timestamp":1591526917902,"user_tz":-120,"elapsed":1073,"user":{"displayName":"myriam dehmas","photoUrl":"https://lh3.googleusercontent.com/-QiP9TxfNrPI/AAAAAAAAAAI/AAAAAAAAB-c/5Z3Xu-PaDwg/s64/photo.jpg","userId":"07724041903811261552"}}},"source":["print(dataset.shape)\n","print(dataset.iloc[dataset.shape[0]-14:dataset.shape[0],])\n","print(\"nombre de bloc de 7 jours: \", dataset.shape[0]/7)  #157"],"execution_count":29,"outputs":[{"output_type":"stream","text":["(1099, 2)\n","           dates    nb\n","1085  21/12/2018  4392\n","1086  22/12/2018  4684\n","1087  23/12/2018  4752\n","1088  24/12/2018  4342\n","1089  25/12/2018  4247\n","1090  26/12/2018  4715\n","1091  27/12/2018  4761\n","1092  28/12/2018  4607\n","1093  29/12/2018  4886\n","1094  30/12/2018  4729\n","1095  31/12/2018  4251\n","1089  25/12/2018  4247\n","1090  26/12/2018  4715\n","1091  27/12/2018  4761\n","nombre de bloc de 7 jours:  157.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iH8wK3X1C8an","colab_type":"text"},"source":["Remarque: les blocs de 7 jours ne sont pas alignés sur une semaine standard,i.e que jour 1 ne correspond pas forcément à un lundi par exemple. D'où la réference bloc de 7 ours et pas une semaine."]},{"cell_type":"markdown","metadata":{"id":"tLwGTbtedYuB","colab_type":"text"},"source":["# ensembles d'entrainement et de test"]},{"cell_type":"code","metadata":{"id":"IzpY6jGbwXMi","colab_type":"code","outputId":"d331b478-72a2-429f-e659-d98fe55e04b0","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1591526921537,"user_tz":-120,"elapsed":1063,"user":{"displayName":"myriam dehmas","photoUrl":"https://lh3.googleusercontent.com/-QiP9TxfNrPI/AAAAAAAAAAI/AAAAAAAAB-c/5Z3Xu-PaDwg/s64/photo.jpg","userId":"07724041903811261552"}}},"source":["train, test = split_dataset(dataset.values)\n","# validate train data\n","print(train.shape)\n","print(\"du \",train[0, 0, 0],\"au\", train[-1, -1, 0])\n","# validate test\n","print(test.shape)\n","print(\"du \",test[0, 0, 0],\"au\", test[-1, -4, 0],\"+ 4 jours\")"],"execution_count":30,"outputs":[{"output_type":"stream","text":["(105, 7, 2)\n","du  01/01/2016 au 04/01/2018\n","(52, 7, 2)\n","du  05/01/2018 au 31/12/2018 + 4 jours\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"clrf35Vse2Sn","colab_type":"text"},"source":["# Execution\n","**Entrées**: fichier csv de la série temporelle.\n","\n","Un modèle sauvegardé peut etre rechargé. \n","Un autre modèle peut etre réajusté:\n","\n","**Paramètres à renseigner** : (dans la cellule Entrainement et évaluation du modèle) \n","\n","- n_input : nombre d'observations antérieures à considérer.\n","- n_nodes : nombre de blocs LSTM\n","- n_epochs : nombre d'epochs\n","- n_batch : taille du batch\n","  \n","Bien que la taille de la séquence prédite peut etre paramétrée (pour le modèle et la fonction to_supervised()) , la fonction fractionnant les données (split_dataset()) devra etre modifiée pour d'autres tailles.\n","La taille de la séquence antérieure peut elle etre paramétrée ( par expl. l'on peut choisir de prédir les 7 jour futurs étant donné 14 observations passées).\n","\n","Les tailles des ensembles d'entrainement et de test ont été fixées de sorte à considérer pratiquement les deux premières années pour l'entrainement et la dernière pour le test, mais aussi de sorte à avoir des blocs de 7 jours dans chaque ensemble, si d'autres proportions sont à choisir, les modifications conséquentes sont à apporter à la fonction split_dataset() notamment.\n","\n","**Sorties**: le modèle peut etre sauvegardé.\n","Ce modèle peut etre invoqué pour calculer les prédictions d'un ensemble de données (ici seule l'évaluation Walk Forward est faite, le RMSE global et les RMSEs partiels, pour chaque jour du bloc de 7 jours sont retournés ) \n","\n","Le modèle retourne les prédictions (t (n_input +1)- t (n_input +7)) pour une séquence antérieure (t(1) - t(n_input))"]},{"cell_type":"markdown","metadata":{"id":"hmI6JfSsdxKs","colab_type":"text"},"source":["# Entrainement et évaluation du modèle"]},{"cell_type":"code","metadata":{"id":"WgUmTeMFKnek","colab_type":"code","outputId":"dccc576b-3258-462a-d71a-ca43028792d5","colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"status":"error","timestamp":1591526932415,"user_tz":-120,"elapsed":5849,"user":{"displayName":"myriam dehmas","photoUrl":"https://lh3.googleusercontent.com/-QiP9TxfNrPI/AAAAAAAAAAI/AAAAAAAAB-c/5Z3Xu-PaDwg/s64/photo.jpg","userId":"07724041903811261552"}}},"source":["n_input = 7\n","n_nodes= 200\n","epochs= 70\n","batch_size= 16   \n","# fit model\n","config= (n_input,n_nodes,epochs, batch_size )\n","model = build_model(train, config)\n","# evaluate model and get scores\n","score, scores = evaluate_model(model,train, test, n_input)\n","# summarize scores\n","summarize_scores('lstm', score, scores)\n","# plot scores\n","days = ['jour 1', 'jour 2', 'jour 3', 'jour 4', 'jour 5', 'jour 6', 'jour 7']\n","pyplot.plot(days, scores, marker='o', label='lstm')\n","pyplot.show()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Epoch 1/70\n","Epoch 2/70\n","Epoch 3/70\n","Epoch 4/70\n","Epoch 5/70\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-2d4a0d0a1ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# evaluate model and get scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-4806edb86875>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(train, config)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Temps d'entrainement =   %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"XJPggPtWdlfX","colab_type":"text"},"source":["# Sauvegarde du modèle"]},{"cell_type":"code","metadata":{"id":"qvpLsfWJVGfS","colab_type":"code","colab":{}},"source":["# serialize model to JSON\n","model_json = model.to_json()\n","with open('/content/drive/My Drive/lstm_multistep.json', \"w\") as json_file:\n","    json_file.write(model_json)\n","# serialize weights to HDF5\n","model.save_weights('/content/drive/My Drive/lstm_multistep.h5')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_spukGNeAi0","colab_type":"text"},"source":["# Chargement du modèle sauvegardé"]},{"cell_type":"code","metadata":{"id":"Swn13Ei3VInM","colab_type":"code","colab":{}},"source":["# load json and create model\n","json_file = open('/content/drive/My Drive/lstm_multistep.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","loaded_model = model_from_json(loaded_model_json)\n","# load weights into new model\n","loaded_model.load_weights(\"/content/drive/My Drive/lstm_multistep.h5\")\n"],"execution_count":0,"outputs":[]}]}